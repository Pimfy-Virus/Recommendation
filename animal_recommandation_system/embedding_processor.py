import pandas as pd
import numpy as np
import openai
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import time
import re
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class GPTEmbeddingProcessor:
    def __init__(self, api_key=None, model="text-embedding-3-large"):
        """
        GPT ì„ë² ë”© ê¸°ë°˜ ë²¡í„°í™” í”„ë¡œì„¸ì„œ
        """
        if api_key:
            openai.api_key = api_key
        
        self.model = model
        self.embeddings = None
        self.processed_df = None
        self.embedding_dim = 3072
        
        print(f"ğŸ¤– GPT ì„ë² ë”© í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”")
        print(f"   - ëª¨ë¸: {model}")
        print(f"   - ì„ë² ë”© ì°¨ì›: {self.embedding_dim}")
    
    def create_embedding_text(self, row):
        """ë™ë¬¼ ì •ë³´ë¥¼ ì„ë² ë”©ìš© ìì—°ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        
        # ê¸°ë³¸ ì •ë³´
        name = row.get('addinfo01', 'ì´ë¦„ë¯¸ì •')
        gender = row.get('addinfo03', 'ì„±ë³„ë¯¸ì •')
        weight = row.get('addinfo07', 'ëª¸ë¬´ê²Œë¯¸ì •')
        age = row.get('addinfo05', 'ë‚˜ì´ë¯¸ì •')
        neuter = row.get('addinfo04', 'ì¤‘ì„±í™”ë¯¸ì •')
        state = row.get('state', 'ìƒíƒœë¯¸ì •')
        kind = row.get('kind', 'ì„ë³´ì¢…ë¥˜ë¯¸ì •')
        
        # ì„±ê²© ë° íŠ¹ì„± ì •ë³´
        personality_tags = row.get('addinfo08', '')
        personality_desc = row.get('addinfo10', '')
        rescue_story = row.get('addinfo09', '')
        additional_info = row.get('addinfo11', '')
        special_needs = row.get('addinfo16', '')
        daily_care = row.get('addinfo20', '')
        
        # í¬ê¸° ë¶„ë¥˜
        try:
            weight_float = float(weight) if weight != 'ëª¸ë¬´ê²Œë¯¸ì •' else None
            if weight_float:
                if weight_float < 10:
                    size = "ì†Œí˜•"
                elif weight_float < 25:
                    size = "ì¤‘í˜•"
                else:
                    size = "ëŒ€í˜•"
            else:
                size = "í¬ê¸°ë¯¸ì •"
        except:
            size = "í¬ê¸°ë¯¸ì •"
        
        # í•´ì‹œíƒœê·¸ ì²˜ë¦¬
        if personality_tags:
            # #ì• êµìŸì´#ì‚¬ëŒì¢‹ì•„ â†’ ì• êµê°€ ë§ê³  ì‚¬ëŒì„ ì¢‹ì•„í•˜ëŠ”
            cleaned_tags = re.sub(r'#([ê°€-í£a-zA-Z0-9]+)', r'\1', personality_tags)
            tag_list = [tag for tag in cleaned_tags.split('#') if tag.strip()]
            personality_text = ', '.join(tag_list) if tag_list else ''
        else:
            personality_text = ''
        
        # ìì—°ì–´ í˜•íƒœë¡œ êµ¬ì„±
        text_parts = []
        
        # ê¸°ë³¸ ì •ë³´ ë¬¸ì¥
        basic_info = f"{name}ëŠ” {gender}ì´ê³  ëª¸ë¬´ê²Œ {weight}kgì¸ {size} ë™ë¬¼ì…ë‹ˆë‹¤."
        text_parts.append(basic_info)
        
        # ìƒíƒœ ì •ë³´
        if state and kind:
            status_info = f"í˜„ì¬ {state} ìƒíƒœì´ë©° {kind}ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤."
            text_parts.append(status_info)
        
        # ì„±ê²© íŠ¹ì„±
        if personality_text:
            personality_info = f"ì„±ê²© íŠ¹ì§•ì€ {personality_text} ì…ë‹ˆë‹¤."
            text_parts.append(personality_info)
        
        # ì„±ê²© ìƒì„¸ ì„¤ëª… (í† í° íš¨ìœ¨ì„± ê³ ë ¤í•˜ì—¬ ì ì ˆíˆ ì¡°ì •)
        if personality_desc:
            desc_text = personality_desc.strip()
            if desc_text:
                text_parts.append(f"ì„±ê²© ì„¤ëª…: {desc_text}")
        
        # êµ¬ì¡° ë°°ê²½ (ì „ì²´ ë‚´ìš© í¬í•¨ - GPTê°€ ì¤‘ìš”í•œ ë¶€ë¶„ ì•Œì•„ì„œ íŒë‹¨)
        if rescue_story:
            story_text = rescue_story.strip()
            if story_text:
                text_parts.append(f"êµ¬ì¡° ë°°ê²½: {story_text}")
        
        # íŠ¹ë³„ ìš”êµ¬ì‚¬í•­ (ì „ì²´ í¬í•¨)
        if special_needs:
            needs_text = special_needs.strip()
            if needs_text:
                text_parts.append(f"íŠ¹ë³„ ìš”êµ¬ì‚¬í•­: {needs_text}")
        
        # ì¼ìƒ ê´€ë¦¬ (ì „ì²´ í¬í•¨)
        if daily_care:
            care_text = daily_care.strip()
            if care_text:
                text_parts.append(f"ì¼ìƒ ê´€ë¦¬: {care_text}")
        
        # ìµœì¢… í…ìŠ¤íŠ¸ ê²°í•©
        final_text = ' '.join(text_parts)
        
        max_length = 30000  
        if len(final_text) > max_length:
            # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ í…ìŠ¤íŠ¸ ìœ ì§€ (ê¸°ë³¸ì •ë³´ > ì„±ê²© > ê¸°íƒ€)
            essential_parts = text_parts[:4]  # ê¸°ë³¸ì •ë³´ + ì„±ê²© ì •ë³´
            essential_text = ' '.join(essential_parts)
            
            if len(essential_text) <= max_length:
                final_text = essential_text
            else:
                final_text = essential_text[:max_length] + "..."
        
        return final_text
    
    def get_embedding(self, text, max_retries=3):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±"""
        for attempt in range(max_retries):
            try:
                response = openai.embeddings.create(
                    model=self.model,
                    input=text,
                    encoding_format="float"
                )
                return response.data[0].embedding
            
            except openai.RateLimitError:
                print(f"   Rate limit reached, waiting {2**attempt} seconds...")
                time.sleep(2**attempt)
            except Exception as e:
                print(f"   Error on attempt {attempt + 1}: {str(e)}")
                if attempt == max_retries - 1:
                    print(f"   Failed to get embedding after {max_retries} attempts")
                    return None
                time.sleep(1)
        
        return None
    
    def get_embeddings_batch(self, texts, batch_size=100):
        """ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„± (ë¹„ìš© ë° ì†ë„ ìµœì í™”)"""
        print(f"\nğŸš€ GPT ì„ë² ë”© ìƒì„± ì¤‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        embeddings = []
        failed_indices = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in tqdm(range(0, len(texts), batch_size), desc="ì„ë² ë”© ìƒì„±"):
            batch_texts = texts[i:i+batch_size]
            
            try:
                # ë°°ì¹˜ ìš”ì²­
                response = openai.embeddings.create(
                    model=self.model,
                    input=batch_texts,
                    encoding_format="float"
                )
                
                # ê²°ê³¼ ì €ì¥
                batch_embeddings = [data.embedding for data in response.data]
                embeddings.extend(batch_embeddings)
                
                # API ìš”ì²­ ì œí•œ ê³ ë ¤
                time.sleep(0.1)  # 100ms ëŒ€ê¸°
                
            except Exception as e:
                print(f"\nâŒ ë°°ì¹˜ {i//batch_size + 1} ì‹¤íŒ¨: {str(e)}")
                
                # ê°œë³„ ì²˜ë¦¬ë¡œ fallback
                print("   ê°œë³„ ì²˜ë¦¬ë¡œ ì¬ì‹œë„...")
                for j, text in enumerate(batch_texts):
                    embedding = self.get_embedding(text)
                    if embedding:
                        embeddings.append(embedding)
                    else:
                        embeddings.append(np.zeros(self.embedding_dim))
                        failed_indices.append(i + j)
                
                time.sleep(1)  # ì‹¤íŒ¨ ì‹œ ë” ê¸´ ëŒ€ê¸°
        
        print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        print(f"   - ì„±ê³µ: {len(embeddings) - len(failed_indices)}ê°œ")
        print(f"   - ì‹¤íŒ¨: {len(failed_indices)}ê°œ")
        
        return np.array(embeddings), failed_indices
    
    def process_animal_data(self, df):
        """ë™ë¬¼ ë°ì´í„° ì „ì²´ ì²˜ë¦¬"""
        print("ğŸ• ë™ë¬¼ ë°ì´í„° ì„ë² ë”© ì²˜ë¦¬ ì‹œì‘")
        print("=" * 50)
        
        # 1. ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±
        print("ğŸ“ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        embedding_texts = []
        
        for idx, row in df.iterrows():
            text = self.create_embedding_text(row)
            embedding_texts.append(text)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if (idx + 1) % 500 == 0:
                print(f"   ì§„í–‰ë¥ : {idx + 1}/{len(df)} ({((idx + 1)/len(df)*100):.1f}%)")
        
        print(f"âœ… í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(embedding_texts)}ê°œ")
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
        text_lengths = [len(text) for text in embedding_texts]
        print(f"   - í‰ê·  ê¸¸ì´: {np.mean(text_lengths):.0f}ì")
        print(f"   - ìµœëŒ€ ê¸¸ì´: {max(text_lengths)}ì")
        
        # 2. GPT ì„ë² ë”© ìƒì„±
        embeddings, failed_indices = self.get_embeddings_batch(embedding_texts)
        
        # 3. ê²°ê³¼ ì €ì¥
        self.embeddings = embeddings
        self.processed_df = df.copy()
        self.processed_df['embedding_text'] = embedding_texts
        
        return embeddings, embedding_texts
    
    def process_user_query(self, user_input):
        """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        
        # ì‚¬ìš©ì ì…ë ¥ì„ ìì—°ì–´ë¡œ ì •ì œ
        processed_query = self.preprocess_user_query(user_input)
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = self.get_embedding(processed_query)
        
        if query_embedding is None:
            print("âŒ ì‚¬ìš©ì ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return None
        
        return np.array(query_embedding)
    
    def preprocess_user_query(self, user_input):
        """ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë³¸ ì •ì œë§Œ ìˆ˜í–‰ (GPTê°€ ì˜ë¯¸ë¥¼ ì•Œì•„ì„œ ì´í•´)"""
        
        # ê¸°ë³¸ ì •ì œë§Œ ìˆ˜í–‰
        query = str(user_input).strip()
        
        # ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        query = re.sub(r'[^\w\sê°€-í£.,!?]', ' ', query)
        
        # ì—°ì† ê³µë°± ì œê±°
        query = re.sub(r'\s+', ' ', query).strip()
        
        # GPTê°€ ì•Œì•„ì„œ ì´í•´í•˜ë¯€ë¡œ ë³„ë„ ë³€í™˜ ë¶ˆí•„ìš”
        return query
    
    def extract_user_preferences(self, user_input):
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ í•˜ë“œ í•„í„° ì¡°ê±´ ì¶”ì¶œ"""
        preferences = {
            'size': None,
            'age': None,
            'gender': None,
            'personality_query': user_input  # ì „ì²´ ì¿¼ë¦¬ëŠ” ì„±ê²© ë§¤ì¹­ìš©
        }
        
        # í¬ê¸° ì¡°ê±´ ì¶”ì¶œ
        if any(keyword in user_input for keyword in ['ì†Œí˜•ê²¬', 'ì†Œí˜•', 'ì‘ì€']):
            preferences['size'] = 'ì†Œí˜•'
        elif any(keyword in user_input for keyword in ['ì¤‘í˜•ê²¬', 'ì¤‘í˜•', 'ì¤‘ê°„']):
            preferences['size'] = 'ì¤‘í˜•'
        elif any(keyword in user_input for keyword in ['ëŒ€í˜•ê²¬', 'ëŒ€í˜•', 'í°']):
            preferences['size'] = 'ëŒ€í˜•'
        
        # ë‚˜ì´ ì¡°ê±´ ì¶”ì¶œ
        if any(keyword in user_input for keyword in ['ì–´ë¦°', 'ìƒˆë¼', 'ì Šì€']):
            preferences['age'] = 'ì–´ë¦°'
        elif any(keyword in user_input for keyword in ['ë‚˜ì´ë§ì€', 'ê³ ë ¹', 'ì‹œë‹ˆì–´']):
            preferences['age'] = 'ê³ ë ¹'
        
        # ì„±ë³„ ì¡°ê±´ ì¶”ì¶œ
        if any(keyword in user_input for keyword in ['ìˆ˜ì»·', 'ë‚¨ì', 'ë‚¨']):
            preferences['gender'] = 'ë‚¨'
        elif any(keyword in user_input for keyword in ['ì•”ì»·', 'ì—¬ì', 'ì—¬']):
            preferences['gender'] = 'ì—¬'
        
        return preferences
    
    def apply_hard_filters(self, df, embeddings, preferences):
        """ë¬¼ë¦¬ì  ì¡°ê±´ìœ¼ë¡œ ë¨¼ì € í•„í„°ë§"""
        # ì¸ë±ìŠ¤ë¥¼ ë¦¬ì…‹í•´ì„œ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë§Œë“¤ê¸°
        df_reset = df.reset_index(drop=True)
        
        mask = pd.Series([True] * len(df_reset))
        
        print(f"ğŸ” í•˜ë“œ í•„í„° ì ìš© ì „: {len(df_reset)}ë§ˆë¦¬")
        
        # í¬ê¸° í•„í„°
        if preferences['size']:
            print(f"   í¬ê¸° ì¡°ê±´: {preferences['size']}")
            
            # ëª¸ë¬´ê²Œ ë°ì´í„° í™•ì¸ ë° ì •ë¦¬
            weights = pd.to_numeric(df_reset['addinfo07'], errors='coerce')
            
            if preferences['size'] == 'ì†Œí˜•':
                weight_mask = weights < 7
                print(f"   ì†Œí˜• ì¡°ê±´ (<7kg): {weight_mask.sum()}ë§ˆë¦¬")
            elif preferences['size'] == 'ì¤‘í˜•':
                weight_mask = (weights >= 7) & (weights < 20)
                print(f"   ì¤‘í˜• ì¡°ê±´ (7-20kg): {weight_mask.sum()}ë§ˆë¦¬")
            elif preferences['size'] == 'ëŒ€í˜•':
                weight_mask = weights >= 20
                print(f"   ëŒ€í˜• ì¡°ê±´ (â‰¥20kg): {weight_mask.sum()}ë§ˆë¦¬")
            
            mask = mask & weight_mask
            print(f"   í¬ê¸° í•„í„° í›„: {mask.sum()}ë§ˆë¦¬")
            
        # ì„±ë³„ í•„í„°
        if preferences['gender']:
            print(f"   ì„±ë³„ ì¡°ê±´: {preferences['gender']}")
            gender_mask = df_reset['addinfo03'] == preferences['gender']
            mask = mask & gender_mask
            print(f"   ì„±ë³„ í•„í„° í›„: {mask.sum()}ë§ˆë¦¬")
        
        # ë‚˜ì´ í•„í„° (ì—°ë„ ê¸°ë°˜)
        if preferences['age']:
            print(f"   ë‚˜ì´ ì¡°ê±´: {preferences['age']}")
            if preferences['age'] == 'ì–´ë¦°':
                age_mask = df_reset['addinfo05'].astype(str).str.contains('2024|2023', na=False)
                print(f"   ì–´ë¦° ë™ë¬¼ (2023-2024): {age_mask.sum()}ë§ˆë¦¬")
            elif preferences['age'] == 'ê³ ë ¹':
                age_mask = df_reset['addinfo05'].astype(str).str.contains('2013|2014|2015|2016', na=False)
                print(f"   ê³ ë ¹ ë™ë¬¼ (2013-2016): {age_mask.sum()}ë§ˆë¦¬")
            else:
                age_mask = pd.Series([True] * len(df_reset))
            
            mask = mask & age_mask
            print(f"   ë‚˜ì´ í•„í„° í›„: {mask.sum()}ë§ˆë¦¬")
        
        # í•„í„°ë§ëœ ê²°ê³¼ ë°˜í™˜ (ì¸ë±ìŠ¤ë„ í•¨ê»˜ ë§ì¶°ì„œ)
        filtered_df = df_reset[mask].reset_index(drop=True)
        filtered_embeddings = embeddings[mask.values]  # numpy ë°°ì—´ ì¸ë±ì‹±
        
        print(f"ğŸ¯ ìµœì¢… í•„í„°ë§ ê²°ê³¼: {len(filtered_df)}ë§ˆë¦¬")
        
        # í•„í„°ë§ ê²°ê³¼ ìƒ˜í”Œ í™•ì¸
        if len(filtered_df) > 0:
            print("   í•„í„°ë§ëœ ë™ë¬¼ ìƒ˜í”Œ:")
            for i in range(min(3, len(filtered_df))):
                row = filtered_df.iloc[i]
                print(f"   - {row['addinfo01']}: {row['addinfo07']}kg, {row['addinfo03']}")
        
        return filtered_df, filtered_embeddings
    
    def find_similar_animals(self, user_query, top_k=5, available_only=True):
        """í•˜ë“œ í•„í„° + ì„±ê²© ìœ ì‚¬ë„ ë§¤ì¹­"""
        
        if self.embeddings is None:
            print("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ” ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬: '{user_query}'")
        
        # 1. ì‚¬ìš©ì ì„ í˜¸ë„ ì¶”ì¶œ
        preferences = self.extract_user_preferences(user_query)
        print(f"ğŸ¯ ì¶”ì¶œëœ ì„ í˜¸ë„: {preferences}")
        
        # 2. ì…ì–‘ ê°€ëŠ¥ ë™ë¬¼ í•„í„°ë§
        if available_only:
            available_mask = self.processed_df['state'] == 'ì„ë³´ê°€ëŠ¥'
            filtered_df = self.processed_df[available_mask].reset_index(drop=True)
            
            # numpy ë°°ì—´ë„ ê°™ì€ ë§ˆìŠ¤í¬ë¡œ í•„í„°ë§
            available_indices = available_mask.values
            filtered_embeddings = self.embeddings[available_indices]
        else:
            filtered_df = self.processed_df.reset_index(drop=True)
            filtered_embeddings = self.embeddings
        
        # 3. í•˜ë“œ í•„í„° ì ìš©
        final_df, final_embeddings = self.apply_hard_filters(
            filtered_df, filtered_embeddings, preferences
        )
        
        print(f"ğŸ“‹ í•„í„°ë§: ì „ì²´ {len(self.processed_df)}ë§ˆë¦¬ â†’ ì…ì–‘ê°€ëŠ¥ {len(filtered_df)}ë§ˆë¦¬ â†’ ì¡°ê±´ë¶€í•© {len(final_df)}ë§ˆë¦¬")
        
        if len(final_df) == 0:
            print("âŒ ì¡°ê±´ì— ë§ëŠ” ë™ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 4. ì„±ê²© ìœ ì‚¬ë„ ê³„ì‚°
        query_embedding = self.process_user_query(user_query)
        if query_embedding is None:
            return []
        
        query_embedding = query_embedding.reshape(1, -1)
        similarities = cosine_similarity(query_embedding, final_embeddings).flatten()
        
        # 5. ìƒìœ„ kê°œ ì¶”ì¶œ
        available_count = min(top_k, len(similarities))
        top_indices = similarities.argsort()[-available_count:][::-1]
        
        # 6. ê²°ê³¼ ì •ë¦¬
        results = []
        for i, idx in enumerate(top_indices):
            # UIDë¥¼ ì‚¬ìš©í•´ì„œ ë§í¬ ìƒì„±
            uid = final_df.iloc[idx].get('uid', '')
            link = f"https://www.pimfyvirus.com/search/01_v/{uid}" if uid else "ë§í¬ ì—†ìŒ"
            
            animal_info = {
                'rank': i + 1,
                'index': idx,
                'uid': uid,
                'link': link,
                'similarity': similarities[idx],
                'name': final_df.iloc[idx]['addinfo01'],
                'gender': final_df.iloc[idx]['addinfo03'],
                'weight': final_df.iloc[idx]['addinfo07'],
                'age': final_df.iloc[idx].get('addinfo05', 'ë‚˜ì´ë¯¸ì •'),
                'neuter': final_df.iloc[idx].get('addinfo04', 'ì¤‘ì„±í™”ë¯¸ì •'),
                'personality_tags': final_df.iloc[idx]['addinfo08'],
                'personality_desc': final_df.iloc[idx].get('addinfo10', ''),
                'rescue_story': final_df.iloc[idx].get('addinfo09', ''),
                'special_needs': final_df.iloc[idx].get('addinfo16', ''),
                'state': final_df.iloc[idx]['state'],
                'kind': final_df.iloc[idx].get('kind', 'ì„ë³´ì¢…ë¥˜ë¯¸ì •'),
            }
            results.append(animal_info)
        
        # 7. ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ ì¶”ì²œ ê²°ê³¼ (ì¡°ê±´ ë§ì¶¤) (ìƒìœ„ {available_count}ê°œ):")
        print("=" * 60)
        
        for result in results:
            print(f"\n{result['rank']}. ğŸ• {result['name']} (ìœ ì‚¬ë„: {result['similarity']:.3f}) ğŸŸ¢")
            print(f"   ğŸ“Š ê¸°ë³¸ì •ë³´: {result['gender']}, {result['weight']}kg, {result['state']}")
            print(f"   ğŸ­ ì„±ê²©íŠ¹ì§•: {result['personality_tags']}")
            print(f"   ğŸ”— ìƒì„¸ì •ë³´: {result['link']}")
            
            # ì„±ê²© ì„¤ëª…ì´ ìˆìœ¼ë©´ í‘œì‹œ
            if result['personality_desc'] and result['personality_desc'].strip():
                desc = result['personality_desc'][:200] + "..." if len(result['personality_desc']) > 200 else result['personality_desc']
                print(f"   ğŸ’­ ì„±ê²©ì„¤ëª…: {desc}")
            
            # íŠ¹ë³„ ìš”êµ¬ì‚¬í•­ì´ ìˆìœ¼ë©´ í‘œì‹œ
            if result['special_needs'] and result['special_needs'].strip():
                print(f"   âš ï¸  íŠ¹ë³„ìš”êµ¬: {result['special_needs']}")
        
        return results
    
    def save_embeddings(self, output_path="animal_embeddings.pkl"):
        """ì„ë² ë”© ë°ì´í„° ì €ì¥"""
        print(f"\nğŸ’¾ ì„ë² ë”© ë°ì´í„° ì €ì¥: {output_path}")
        
        data = {
            'embeddings': self.embeddings,
            'dataframe': self.processed_df,
            'model': self.model,
            'embedding_dim': self.embedding_dim,
            'metadata': {
                'total_animals': len(self.processed_df),
                'embedding_shape': self.embeddings.shape,
                'model_used': self.model
            }
        }
        
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ")
    
    def load_embeddings(self, file_path="animal_embeddings.pkl"):
        """ì €ì¥ëœ ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“‚ ì„ë² ë”© ë°ì´í„° ë¡œë”©: {file_path}")
        
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        self.embeddings = data['embeddings']
        self.processed_df = data['dataframe']
        self.model = data['model']
        self.embedding_dim = data['embedding_dim']
        
        print(f"âœ… ë¡œë”© ì™„ë£Œ: {data['metadata']}")
    
    def get_recommendation_stats(self):
        """ì¶”ì²œ ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
        if self.processed_df is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ“Š ì¶”ì²œ ì‹œìŠ¤í…œ í†µê³„")
        print("=" * 40)
        
        total_animals = len(self.processed_df)
        available_animals = len(self.processed_df[self.processed_df['state'] == 'ì„ë³´ê°€ëŠ¥'])
        
        print(f"ğŸ• ì „ì²´ ë™ë¬¼ ìˆ˜: {total_animals:,}ë§ˆë¦¬")
        print(f"âœ… ì…ì–‘ ê°€ëŠ¥: {available_animals:,}ë§ˆë¦¬")
        print(f"ğŸ“ˆ ì…ì–‘ ê°€ëŠ¥ ë¹„ìœ¨: {available_animals/total_animals*100:.1f}%")
        
        # ìƒíƒœë³„ ë¶„í¬
        state_counts = self.processed_df['state'].value_counts()
        print(f"\nğŸ“‹ ìƒíƒœë³„ ë¶„í¬:")
        for state, count in state_counts.items():
            percentage = count/total_animals*100
            print(f"   {state}: {count:,}ë§ˆë¦¬ ({percentage:.1f}%)")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
    import os
    api_key = os.getenv('OPENAI_API_KEY') or "your-api-key-here"
    
    # GPT ì„ë² ë”© í”„ë¡œì„¸ì„œ ìƒì„±
    gpt_processor = GPTEmbeddingProcessor(
        api_key=api_key,
        model="text-embedding-3-small"  # ë˜ëŠ” "text-embedding-3-large"
    )
    
    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('homeprotection_data.csv')
    
    # ì„ë² ë”© ìƒì„± (ì‹œê°„ ì†Œìš”)
    embeddings, texts = gpt_processor.process_animal_data(df)
    
    # ì„ë² ë”© ì €ì¥
    gpt_processor.save_embeddings()
    
    # ì¶”ì²œ í…ŒìŠ¤íŠ¸
    results = gpt_processor.find_similar_animals("í™œë°œí•˜ê³  ì• êµ ë§ì€ ì†Œí˜•ê²¬ì„ ì›í•´ìš”", available_only=True)
    
    print("\nğŸ¯ GPT ì„ë² ë”© ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")